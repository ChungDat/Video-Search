{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ab3775",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e74f6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 18:44:18.418 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from app.utils import get_video_pack_files, get_keyframe_data\n",
    "from app.PATH import CLIP_FEATURES_PATH, MAP_KEYFRAMES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78275f",
   "metadata": {},
   "source": [
    "## Create Qdrant client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abd7144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qdrant_client.qdrant_client.QdrantClient at 0x1dbc0b384d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225963cb",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c5beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PACK_LIST = [pack for pack in range(21, 31)]\n",
    "COLLECTION_NAME = \"my_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1dd42",
   "metadata": {},
   "source": [
    "## Create colletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc10f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'my_collection' created.\n"
     ]
    }
   ],
   "source": [
    "if COLLECTION_NAME not in [c.name for c in client.get_collections().collections]:\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(size=512, distance=models.Distance.COSINE),\n",
    "        optimizers_config=models.OptimizersConfigDiff(\n",
    "            indexing_threshold=0,\n",
    "        ),\n",
    "    )\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created.\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a8254",
   "metadata": {},
   "source": [
    "## Upsert data automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90118509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L21_V001.npy', 'L21_V002.npy', 'L21_V003.npy', 'L21_V005.npy', 'L21_V006.npy']\n",
      "Process L21\n",
      "Process L22\n",
      "Process L23\n",
      "Process L24\n",
      "Process L25\n",
      "Process L26\n",
      "Process L27\n",
      "Process L28\n",
      "Process L29\n",
      "Process L30\n"
     ]
    }
   ],
   "source": [
    "start_id = 0\n",
    "process_file = 0\n",
    "if not os.path.exists(CLIP_FEATURES_PATH):\n",
    "    print(\"The folder does not exist\")\n",
    "else:\n",
    "    start_id = 0\n",
    "    print(os.listdir(CLIP_FEATURES_PATH)[:5])\n",
    "    for pack in VIDEO_PACK_LIST:\n",
    "        video_pack = f\"L{pack}\"\n",
    "        print(f\"Process {video_pack}\")\n",
    "        files_per_video_pack = get_video_pack_files(CLIP_FEATURES_PATH, video_pack)\n",
    "        # A list of .npy files, each file contains a list of 512 features vectors, \n",
    "        # each vector is embedded from a keyframe of the video that has the same name as the file\n",
    "\n",
    "        if len(files_per_video_pack) == 0:\n",
    "            print(f\"No files found for video pack {video_pack}\")\n",
    "            continue\n",
    "\n",
    "        for file in files_per_video_pack:\n",
    "            # Process each video features in the pack\n",
    "            feature = np.load(os.path.join(CLIP_FEATURES_PATH, file))\n",
    "            if feature.shape[1] != 512:\n",
    "                print(f\"File {file} does not have 512 features, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Insert the feature into the collection\n",
    "            file_name = file[:-4]\n",
    "            num_frames = feature.shape[0]\n",
    "            df = get_keyframe_data(MAP_KEYFRAMES_PATH, file_name)\n",
    "            payloads = [{\n",
    "                \"origin\": file_name[:3], \n",
    "                \"video\": file_name[4:],\n",
    "                \"frame_index\": df.iloc[i][\"frame_idx\"], \n",
    "                \"frame\": f\"00{i + 1}.jpg\" if i + 1 < 10 else f\"0{i + 1}.jpg\" if i + 1 < 100 else f\"{i + 1}.jpg\"\n",
    "                }\n",
    "                for i in range(num_frames)\n",
    "            ]\n",
    "\n",
    "            client.upsert(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                points=models.Batch(\n",
    "                    ids=range(start_id, start_id + feature.shape[0]),\n",
    "                    vectors=feature.tolist(),\n",
    "                    payloads=payloads\n",
    "                )\n",
    "            )\n",
    "            start_id += feature.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aed302",
   "metadata": {},
   "source": [
    "## Upsert data manually (1 pack at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772795a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2637191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_pack = \"L30\" # manually iterate from this list [\"L21\", \"L22\", \"L23\", ..., \"L30\"]\n",
    "\n",
    "# files_per_video_pack = get_video_pack_files(CLIP_FEATURES_PATH, video_pack)\n",
    "# # A list of .npy files, each file contains a list of 512 features vectors,\n",
    "# # each vector is embedded from a keyframe of the video that has the same name as the file\n",
    "\n",
    "# if len(files_per_video_pack) == 0:\n",
    "#     print(f\"No files found for video pack {video_pack}\")\n",
    "# else:\n",
    "#     for file_name in files_per_video_pack:\n",
    "#         feature = np.load(os.path.join(CLIP_FEATURES_PATH, file_name))\n",
    "#         if feature.shape[1] != 512:\n",
    "#             print(f\"File {file_name} does not have 512 features, skipping\")\n",
    "#             continue\n",
    "#         print(f\"Processing file {file_name} with shape {feature.shape}\")\n",
    "\n",
    "#         # Insert the feature into the collection\n",
    "#         num_frames = feature.shape[0]\n",
    "#         payloads = [\n",
    "#             {\"origin\": file_name[:-4], \"frame_id\": i + 1}\n",
    "#             for i in range(num_frames)\n",
    "#         ]\n",
    "\n",
    "#         client.upsert(\n",
    "#             collection_name=COLLECTION_NAME,\n",
    "#             points=models.Batch(\n",
    "#                 ids=range(start_id, start_id + feature.shape[0]),\n",
    "#                 vectors=feature.tolist(),\n",
    "#                 payloads=payloads\n",
    "#             )\n",
    "#         )\n",
    "#         start_id += feature.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b3b01",
   "metadata": {},
   "source": [
    "## Check collection status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f30dd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=177321 points_count=177321 segments_count=5 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=512, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfigOutput(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, max_points_count=None, filter_max_conditions=None, condition_max_size=None, multivector_config=None, sparse_config=None)) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "print(client.get_collection(COLLECTION_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf91295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points with no vectors: 0\n"
     ]
    }
   ],
   "source": [
    "def count_points_without_vectors(client, collection_name):\n",
    "    offset = None\n",
    "    count = 0\n",
    "    while True:\n",
    "        points, offset = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            with_vectors=True,\n",
    "            offset=offset,\n",
    "            limit=1000  # batch size\n",
    "        )\n",
    "        for point in points:\n",
    "            if point.vector is None:\n",
    "                count += 1\n",
    "        if offset is None:\n",
    "            break\n",
    "    return count\n",
    "\n",
    "no_vector_count = count_points_without_vectors(client, COLLECTION_NAME)\n",
    "print(f\"Points with no vectors: {no_vector_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a09ee",
   "metadata": {},
   "source": [
    "## Enable indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d85dc805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.update_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=20000),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55658b2e",
   "metadata": {},
   "source": [
    "## Delete collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b6006ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_collection(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e851b85",
   "metadata": {},
   "source": [
    "## Delete all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5e44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=873, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client.delete(\n",
    "#     collection_name=COLLECTION_NAME,\n",
    "#     points_selector=models.FilterSelector(\n",
    "#         filter=models.Filter(  # match all points\n",
    "#             must=[]\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6e986",
   "metadata": {},
   "source": [
    "## Count all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a87893c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=177321)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    count_filter=models.Filter(\n",
    "        must=[]\n",
    "    ),\n",
    "    exact=True,\n",
    ")\n",
    "# 177321"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd5046",
   "metadata": {},
   "source": [
    "## Retrive a list of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeda94ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Record(id=0, payload={'origin': 'L21', 'video': 'V001', 'frame_index': 0.0, 'frame': '001.jpg'}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=9, payload={'origin': 'L21', 'video': 'V001', 'frame_index': 1131.0, 'frame': '010.jpg'}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=99, payload={'origin': 'L21', 'video': 'V001', 'frame_index': 12450.0, 'frame': '100.jpg'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.retrieve(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    ids=[0, 9, 99],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae30386",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.search(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query_vector=query_vector, # assuming query_vector is defined, a python list of 512 features (float)\n",
    "    with_payload=True,\n",
    "    with_vectors=True,\n",
    "    limit=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd6bc8",
   "metadata": {},
   "source": [
    "## Verify Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00569fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoTokenizer, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42263244",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(os.path.join(os.getcwd(), \"keyframes\", \"L23_V001\", \"001.jpg\"))\n",
    "image = image.convert(\"RGB\")  # Ensure the image is in RGB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b00677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0011, 0.0084, 0.9905]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(images=image, text=[\"a group of bikers\", \"a man holding a sign of number 2\", \"a cow and a woman are waving\"], return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1417,  0.1083,  0.1551,  ...,  0.1857, -0.1058, -0.1027],\n",
      "        [ 0.2465, -0.0720, -0.2029,  ..., -0.8686,  0.1537, -0.3400],\n",
      "        [ 0.3818,  0.2169, -0.2657,  ..., -0.1489,  0.2350, -0.2732]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "torch.Size([3, 512])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "inputs = tokenizer([\"a group of bikers\", \"a man holding a sign of number 2\", \"a cow and a woman are waving\"], return_tensors=\"pt\", padding=True)\n",
    "text_features = model.get_text_features(**inputs)\n",
    "print(text_features.shape)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "image_features = model.get_image_features(**inputs)\n",
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde0420",
   "metadata": {},
   "source": [
    "Recreate embedding feature from L23_V001/001.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76591eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_filter = models.Filter(\n",
    "#     must=[\n",
    "#         models.FieldCondition(\n",
    "#             key=\"origin\",\n",
    "#             match=models.MatchValue(value=\"L23_V001\")\n",
    "#         )\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271de93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points, next_page = client.scroll(\n",
    "#     COLLECTION_NAME,\n",
    "#     limit=10,\n",
    "#     with_payload=True,\n",
    "#     with_vectors=True,\n",
    "#     scroll_filter=query_filter,    \n",
    "# )\n",
    "# print(points[0].payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f29af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(os.path.join(os.getcwd(), \"keyframes\", \"L23_V001\", \"001.jpg\"))\n",
    "image = image.convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa0901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "tensor([[[ 0.5581, -0.1280, -0.3032, -0.2594, -0.2594],\n",
      "         [ 1.1128,  0.5143,  0.3537,  0.3245,  0.2953],\n",
      "         [ 0.5727,  0.5581,  0.3975,  0.3391,  0.2661],\n",
      "         [ 0.5581,  0.4413,  0.2953,  0.2953,  0.2807],\n",
      "         [ 0.5289,  0.3391,  0.2077,  0.2369,  0.2077]],\n",
      "\n",
      "        [[-0.1613, -0.5365, -0.5515, -0.5665, -0.5665],\n",
      "         [ 0.2439, -0.0262,  0.0338,  0.0338,  0.0038],\n",
      "         [-0.3714, -0.1012,  0.0338,  0.0789,  0.0338],\n",
      "         [-0.4014, -0.2363, -0.0862,  0.0038,  0.0488],\n",
      "         [-0.3714, -0.2663, -0.1613, -0.0712, -0.0112]],\n",
      "\n",
      "        [[-0.0013, -0.4137, -0.4564, -0.4279, -0.4564],\n",
      "         [ 0.4679,  0.0840,  0.0555,  0.0271, -0.0298],\n",
      "         [-0.0867,  0.0555,  0.0129, -0.0440, -0.1293],\n",
      "         [-0.1009, -0.1009, -0.1293, -0.1009, -0.1435],\n",
      "         [-0.0867, -0.1578, -0.2431, -0.1862, -0.2004]]])\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "print(inputs[\"pixel_values\"].shape)   # should be (1, 3, 224, 224)\n",
    "print(inputs[\"pixel_values\"][0, :, :5, :5])  # peek at the first 5Ã—5 patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be052219",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = model.get_image_features(**inputs)\n",
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c582771",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = np.load(os.path.join(os.getcwd(), \"clip-features-32\", \"L23_V001.npy\"))  # given embedding\n",
    "ref = ref[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c494774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 1.0000551\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    new_emb = model.get_image_features(**inputs).cpu().numpy()\n",
    "\n",
    "# Normalize both (in case ref was normalized)\n",
    "ref = ref / np.linalg.norm(ref)\n",
    "new_emb = new_emb / np.linalg.norm(new_emb)\n",
    "\n",
    "# Compare\n",
    "cosine_sim = np.dot(ref, new_emb.T).squeeze()\n",
    "print(\"Cosine similarity:\", cosine_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
